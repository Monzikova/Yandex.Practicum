# Классификация комментариев с BERT

## Предполагаемый заказчик

Интернет-магазин

## Цель проекта

Проект по машинному обучению. Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок. Постройте модель со значением метрики качества F1 не меньше 0.75.

## Описание проекта

В нашем распоряжении был набор данных с разметкой о токсичности правок. Комментариев с негативным окрасом в данных сильно меньше чем позитивных, а значит классы несбалансированны. Поскольку в данных почти 160 тысяч комментариев, для сокращения времени обучения мы использовали лишь небольшую часть всех данных - 5000 комментариев на тренировочной и 2000 комментраие на тестовой выборках. На тренировочной выборке мы сохранили исходный баланс классов, а на тестовой изменили, сделав равное количество негативных и позитивных комментариев.

Во время предобработки данных мы убрали ненужные символы переноса строки, цифры, знаки препинания и и лемматизировали слова, используя библиотеку spacy.

Мы использовали два подхода к созданию эмбединга слов: TF-IDF и модель BERT.

Наилучший результат с векторами TF-IDF показали модели логистической регресси и градиентного бустинга, однако нам не удалось добиться метрики F1 превышающей заданное минимальное значение в 0.75.

Второй подход с использованием предобученной модели BERT оказался более удачным. Мы использовали модель 'unitary/toxic-bert', созданную специально для выявления негативных комментариев. В отличии от подхода с TF-IDF, все модели достигли нужной метрики со стандартными значениями. После подбора гиперпараметров, лучший результат получился у модели случайного леса, метрика F1 получилась равной 0.9 на тренировочной выборке.

Выбранная модель случайного леса обученная на эмбеддингах модели BERT показала на тренировочной выборке результат для F1 равный 0.93, что удовлетворяет условию поставленной задачи.

## Инструменты

`Python`, `Pandas` , `Scikit-learn` , `BERT`, `nltk`, `tf-idf`, `CatBoost`, `LigthGBM`, `Matplotlib` , `Seaborn`, `BERT`, `nltk`, `tf-idf`

## Ключевые слова проекта

`обработка естественного языка`, `NLP`
